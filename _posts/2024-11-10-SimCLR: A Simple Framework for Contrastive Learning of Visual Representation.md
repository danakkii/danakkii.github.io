# SimCLR
A Simple Framework for Contrastive Learning of Visual Representations


## Abstract
- SimCLRì€ unsupervised learning algorithmìœ¼ë¡œ ì´ë¯¸ì§€ ë°ì´í„°ì— labelì´ ì—†ëŠ” ìƒí™©ì—ì„œ visual representationì„ ì¶”ì¶œí•˜ì—¬ downstream taskë¥¼ í•´ê²°í•˜ê³ ì í•¨
- Data augmentationì„ í†µí•´ ì–»ì€ positive/negative sampleì— ëŒ€í•´ contrastive learningì„ ì ìš©(positive pairë¼ë¦¬ëŠ” ê°™ê²Œ, negative pair ë¼ë¦¬ëŠ” ë‹¤ë¥´ê²Œ) ïƒ  ê°™ì€ ë°ì´í„°ì— ì—¬ëŸ¬ ë°©ì‹ìœ¼ë¡œ ë³€í˜•í•˜ì—¬ ì–»ì€ ì ì¬ ë²¡í„°ê°€ ì„œë¡œ ì¼ì¹˜í•˜ë„ë¡ í•™ìŠµ

## Method
### The Contrastive Learning Framework
![simclr_1](https://github.com/danakkii/Paper/assets/117612063/26df35ab-6e0e-4737-bcd5-ac83448fdd24)

1.  ğ‘¥ â†’  ğ‘‘ğ‘ğ‘¡ğ‘ ğ‘ğ‘¢ğ‘”ğ‘šğ‘’ğ‘›ğ‘¡ğ‘ğ‘¡ğ‘–ğ‘œğ‘›("tâ€™ ~ " Ï„)â†’ ğ‘¥Â Ìƒ"i, " ğ‘¥Â Ìƒ"j" (positive pair)
* input = minibatch

2. f(ğ‘¥Â Ìƒ"i") â†’ Encoding(ResNet(ğ‘¥Â Ìƒi)) â†’ hi 
* extracts representation vectors(=encoder) 

3. g("hi" )â†’ğ‘¤^((2))  "Ïƒ(" ğ‘¤^((1)) "hi)"â†’ğ‘§ğ‘–â†’ğ‘™ğ‘œğ‘ ğ‘  ğ‘šğ‘–ğ‘›ğ‘–ğ‘šğ‘¢ğ‘š
*  Maps representations, Ïƒ = ReLU non-linearity
*  Vector representationì˜ ìœ ì‚¬ì„±ì€ ìµœëŒ€í™”, contrastive loss functionì€ ìµœì†Œí™”  


![simclr_4](https://github.com/danakkii/Paper/assets/117612063/c626131e-7630-4bab-868e-3daea4a3fb6d)
### NX - Xent Loss Function(The Contrastive Loss function)
- Cross-entropy ê¸°ë°˜
- ë²¡í„° ì‚¬ì´ì˜ ìœ ì‚¬ì„± ë¹„êµë¥¼ ìœ„í•´ ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ì‚¬ìš© -> ë‘ ë²¡í„°ì˜ ë°©í–¥ì´ ê°™ì€ ê²½ìš° 1, ë°˜ëŒ€ì¼ ê²½ìš° -1
- ì´ë¥¼ ì´ìš©í•´ positive pariì— ëŒ€í•œ ìœ ì‚¬ì„± NX-Xent Loss functionì„ ì‚¬ìš©í•˜ì—¬ ì •ì˜
![ê·¸ë¦¼1](https://github.com/user-attachments/assets/5cfafe43-bd5c-4338-a031-a47871027641)
![ê·¸ë¦¼2](https://github.com/user-attachments/assets/cdb49379-bc47-448d-8bbb-8e1618e61690)


### Algorithm 1 - SimCLR's main learning algorithm
![simclr_2](https://github.com/danakkii/Paper/assets/117612063/4b21e586-b08a-4786-95ef-cb92fea2653f)

### Experiment
-  Defaul setting
Data augmentation : random crop and resize (with random flip), color distortions, and Gaussian blur
Encoder : ResNet-50, 2-layer MLP projection head
           *To project the representation to a 128-dimensional latent space
Loss : NT-Xent, LARS optimizer
* With learning rate of 4.8(=0.3 x batch size/256), weight decay of 10-6
Batch size : 4096 for 100 epochs

### Conclusion
Color + Crop transformer ì¡°í•©ì´ ê°€ì¥ ì„±ëŠ¥ì´ ì¢‹ìŒ
Contrastive learningì˜ ë‹¨ì ì¸ Negative pairê°€ ë§ì´ í•„ìš”í•˜ë‹¤ëŠ” ì ì„ batch sizeë¥¼ í¬ê²Œ í•˜ì—¬ í•´ê²°í•˜ì˜€ì§€ë§Œ batch sizeê°€ ì»¤ì•¼ ì„±ëŠ¥ì´ ì¢‹ë‹¤ëŠ” í•œê³„ì  ì¡´ì¬



